#!/usr/bin/env python

import sys, os, argparse, subprocess, shutil, textwrap
from pipits import runcmd as rc
from pipits import tcolours as tc

__author__ = "Hyun Soon Gweon"
__copyright__ = "Copyright 2015, The PIPITS Project"
__credits__ = ["Hyun Soon Gweon", "Anna Oliver", "Joanne Taylor", "Tim Booth", "Melanie Gibbs", "Daniel S. Read", "Robert I. Griffiths", "Karsten Schonrogge"]
__license__ = "GPL"
__maintainer__ = "Hyun Soon Gweon"
__email__ = "hyugwe@ceh.ac.uk"

VSEARCH = "vsearch"
BIOM = "biom"

if __name__ == '__main__':

    parser = argparse.ArgumentParser("PIPITS_PROCESS: Sequences to OTU Table")
    parser.add_argument(
        "-i",
        action = "store",
        dest = "input",
        metavar = "<FILE>",
        help = "[REQUIRED] ITS sequences in FASTA. Typically output from pipits_funits",
        required = True)
    parser.add_argument(
        "-o",
        action = "store",
        dest = "outDir",
        metavar = "<DIR>",
        default = "pipits_process",
        help = "[REQUIRED] Directory to output results.",
        required = False)
    parser.add_argument(
        "-d",
        action = "store",
        dest = "VSEARCH_id",
        metavar = "<FLOAT>",
        help = "VSEARCH - Identity threshold [default: 0.97]",
        default = "0.97",
        required = False)
    parser.add_argument(
        "-c",
        action = "store",
        dest = "RDP_assignment_threshold",
        metavar = "<FLOAT>",
        help = "RDP assignment confidence threshold - RDP Classifier confidence threshold for output [default: 0.85]",
        default = "0.85",
        required = False)
    parser.add_argument(
        "-l",
        action = "store",
        dest = "sortedlist",
        metavar = "<TXT>",
        help = "[REQUIRED] Sample list file. Generated with PIPITS_GETREADPAIRSLIST prior to PIPITS_PREP",
        required = False)
    parser.add_argument(
        "--includeuniqueseqs",
        action = "store_true",
        dest = "includeuniqueseqs",
        help = "[REQUIRED] PIPITS by default removes unique sequences before clustering. This means you wouldn't have any singletons. If you want singletons, then choose this option. It can take much longer to process.",
        required = False)
    # Generic
    parser.add_argument(
        "-r",
        action = "store_true",
        dest = "retain",
        help = "Retain intermediate files (Beware intermediate files use excessive disk space!)",
        required = False)
    parser.add_argument(
        "-v",
        action = "store_true",
        dest = "verbose",
        help = "Verbose mode",
        required = False)
    parser.add_argument(
        "-t",
        action = "store",
        dest = "threads",
        metavar = "<INT>",
        help = "Number of Threads [default: 1]",
        default = "1",
        required = False)
    parser.add_argument(
        "--Xms",
        action = "store",
        dest = "Xms",
        metavar = "<INT>",
        help = "The minimum size, in bytes, of the memory allocation pool for JVM",
        required = False)
    parser.add_argument(
        "--Xmx",
        action = "store",
        dest = "Xmx",
        metavar = "<INT>",
        help = "The maximum size, in bytes, of the memory allocation pool for JVM",
        required = False)
    options = parser.parse_args()


    EXE_DIR = os.path.dirname(os.path.realpath(__file__))
    if not os.path.exists(options.outDir):
        os.mkdir(options.outDir)
    else:
        shutil.rmtree(options.outDir)
        os.mkdir(options.outDir)
    tmpDir = options.outDir + "/intermediate"
    if not os.path.exists(tmpDir):
        os.mkdir(tmpDir)


    # Logging
    import logging
    logger = logging.getLogger("pipits_process")
    logger.setLevel(logging.DEBUG)

    streamLoggerFormatter = logging.Formatter("%(asctime)s %(levelname)s: %(message)s", tc.HEADER + "%Y-%m-%d %H:%M:%S" + tc.ENDC)

    streamLogger = logging.StreamHandler()
    if options.verbose:
        streamLogger.setLevel(logging.DEBUG)
    else:
        streamLogger.setLevel(logging.INFO)
    streamLogger.setFormatter(streamLoggerFormatter)
    logger.addHandler(streamLogger)

    fileLoggerFormatter = logging.Formatter("%(asctime)s %(levelname)s: %(message)s", tc.HEADER + "%Y-%m-%d %H:%M:%S" + tc.ENDC)
    fileLogger = logging.FileHandler(options.outDir + "/log.txt", "w")
    fileLogger.setLevel(logging.DEBUG)
    fileLogger.setFormatter(fileLoggerFormatter)
    logger.addHandler(fileLogger)


    # Start
    logger.info(tc.OKBLUE + "PIPITS PROCESS started" + tc.ENDC)

    # Check for input file
    if not os.path.exists(options.input):
        logger.error("Error: Input file doesn't exist")
        exit(1)

    if os.stat(options.input).st_size == 0:
        logger.error("Input file is empty!")
        exit(0)


    # Sample ids
    if not options.sortedlist:

        # Generating a sample list
        logger.info("Generating a sample list from the input sequences")

        try:
            os.remove(tmpDir + "/sampleIDs.txt")
        except OSError:
            pass

        cmd = " ".join(["python", EXE_DIR + "/pipits_getsamplelistfromfasta",
                        "-i", options.input,
                        "-o", options.outDir + "/sampleIDs.txt"])
        rc.run_cmd(cmd, logger, options.verbose)
        options.sortedlist = options.outDir + "/sampleIDs.txt"


    sampleIDs = []
    if not os.path.exists(options.sortedlist):
        print("Error: Sample list file does NOT exist")
        exit(1)

    for line in open(options.sortedlist):
        if line[0] != "#":
            sampleIDs.append(line.split("\t")[0].rstrip())

    # Check for duplicate entries
    duplicateIDs = list(set([x for x in sampleIDs if sampleIDs.count(x) > 1]))
    if len(duplicateIDs) > 0:
        print("Error: You have a duplicate id in your Sample list file. Offending id: " + ",".join(duplicateIDs))
        exit(1)


        
    # Derep
    logger.info("Dereplicating and removing unique sequences prior to picking OTUs")
    minuniquesize = 2
    if options.includeuniqueseqs:
        minuniquesize = 1
    cmd = " ".join([VSEARCH, "--derep_fulllength", options.input, 
                    "--output", tmpDir + "/input_nr.fasta", 
                    "--minuniquesize", str(minuniquesize), 
                    "--sizeout",
                    "--threads", options.threads])
    rc.run_cmd_VSEARCH(cmd, logger, options.verbose)

    # Check if the file is empty
    if os.stat(tmpDir + "/input_nr.fasta").st_size == 0:
        logger.info(tc.OKYELLOW + "After dereplicating and removing unique sequences, there aren't no sequences! Processing stopped." + tc.ENDC)
        exit(0)


    # OTU clustering
    logger.info("Picking OTUs [VSEARCH]")
    cmd = " ".join([VSEARCH, 
                    "--cluster_fast", tmpDir + "/input_nr.fasta", 
                    "--id", options.VSEARCH_id,
                    "--centroids", tmpDir + "/input_nr_otus.fasta",
                    "--uc", tmpDir + "/input_nr_otus.uc",
                    "--threads", options.threads])
    rc.run_cmd_VSEARCH(cmd, logger, options.verbose)


    # Chimera removal
    logger.info("Removing chimeras [VSEARCH]")
    cmd = " ".join([VSEARCH, 
                    "--uchime_ref", tmpDir + "/input_nr_otus.fasta", 
                    "--db $PIPITS_UNITE_REFERENCE_DATA_CHIMERA",
                    "--nonchimeras", tmpDir + "/input_nr_otus_nonchimeras.fasta",
                    "--threads", options.threads])
    rc.run_cmd_VSEARCH(cmd, logger, options.verbose)


    # Rename OTUs
    logger.info("Renaming OTUs")
    def renumberOTUS():
        handle_in = open(tmpDir + "/input_nr_otus_nonchimeras.fasta", "rU")
        handle_out = open(tmpDir + "/input_nr_otus_nonchimeras_relabelled.fasta", "w")

        counter = 1
        
        for line in handle_in:
            if line.startswith(">"):
                newlabel = line[1:].split(";")[0]
                handle_out.write(">OTU" + str(counter) + "\n")
                counter += 1
            else:
                handle_out.write(line.rstrip() + "\n")
        handle_in.close()
        handle_out.close()

    renumberOTUS()


    # Map reads to OTUs
    logger.info("Mapping reads onto centroids [VSEARCH]")
    cmd = " ".join([VSEARCH, 
                    "--usearch_global", options.input, 
                    "--db", tmpDir + "/input_nr_otus_nonchimeras_relabelled.fasta", 
                    "--id", options.VSEARCH_id, 
                    "--uc", tmpDir + "/otus.uc",
                    "--threads", options.threads])
    rc.run_cmd_VSEARCH(cmd, logger, options.verbose)


    # OTU construction
    logger.info("Making OTU table")
    cmd = " ".join(["python", EXE_DIR + "/pipits_uc2otutable", 
                    "-i", tmpDir + "/otus.uc", 
                    "-o", tmpDir + "/otu_table_prelim.txt",
                    "-l", options.sortedlist])
    rc.run_cmd_VSEARCH(cmd, logger, options.verbose)

    # Convert to biom
    logger.info("Converting classic tabular OTU into a BIOM format [BIOM]")
    try:
        os.remove(tmpDir + "/otu_table_prelim.biom")
    except OSError:
        pass
    cmd = " ".join([BIOM, "convert", 
                    "-i", tmpDir + "/otu_table_prelim.txt", 
                    "-o", tmpDir + "/otu_table_prelim.biom", 
                    "--table-type=\"OTU table\" --to-json"])
    rc.run_cmd(cmd, logger, options.verbose)


    # Classifying OTUs
    # http://sourceforge.net/projects/rdp-classifier/files/RDP_Classifier_TrainingData/ 
    JVM_memory_option = ""
    if options.Xms:
        JVM_memory_option = JVM_memory_option + "-Xms" + str(options.Xms) + " "

    if options.Xmx:
        JVM_memory_option = JVM_memory_option + "-Xmx" + str(options.Xmx) + " "
    
    logger.info("Assigning taxonomy [RDP Classifier]")
    cmd = " ".join(["java", JVM_memory_option, "-jar $PIPITS_RDP_CLASSIFIER_JAR classify", 
                    "-t $PIPITS_UNITE_RETRAINED_DIR" + "/rRNAClassifier.properties", 
                    "-o", options.outDir + "/assigned_taxonomy.txt", 
                    tmpDir + "/input_nr_otus_nonchimeras_relabelled.fasta"])
    rc.run_cmd(cmd, logger, options.verbose)


    # Reformatting RDP_CLASSIFIER output for biom
    logger.info("Reformatting RDP_Classifier output")
    cmd = " ".join(["python", EXE_DIR + "/pipits_reformatAssignedTaxonomy", 
                    "-i", options.outDir + "/assigned_taxonomy.txt" , 
                    "-o", options.outDir + "/assigned_taxonomy_reformatted_filtered.txt",
                    "-c", options.RDP_assignment_threshold])
    rc.run_cmd(cmd, logger, options.verbose)


    # Adding RDP_CLASSIFIER output to OTU table
    logger.info("Adding assignment to OTU table [BIOM]")
    try:
        os.remove(options.outDir + "/otu_table.biom")
    except OSError:
        pass
    cmd = " ".join([BIOM, "add-metadata", 
                    "-i", tmpDir + "/otu_table_prelim.biom", 
                    "-o", options.outDir + "/otu_table.biom", 
                    "--observation-metadata-fp", options.outDir + "/assigned_taxonomy_reformatted_filtered.txt", 
                    "--observation-header", "OTUID,taxonomy,confidence", 
                    "--sc-separated", "taxonomy", 
                    "--float-fields", "confidence", 
                    "--output-as-json"])
    rc.run_cmd(cmd, logger, options.verbose)


    # Convert BIOM to TABLE
    logger.info("Converting OTU table with taxa assignment into a BIOM format [BIOM]")
    try:
        os.remove(options.outDir + "/otu_table.txt")
    except OSError:
        pass
    cmd = " ".join([BIOM, "convert", 
                    "-i", options.outDir + "/otu_table.biom", 
                    "-o", options.outDir + "/otu_table.txt",
                    "--to-tsv",
                    "--header-key taxonomy"])
    rc.run_cmd(cmd, logger, options.verbose)


    # Make phylotyp table
    logger.info("Phylotyping OTU table")
    cmd = " ".join(["python", EXE_DIR + "/pipits_phylotype_biom", "-i", options.outDir + "/otu_table.biom", "-o", options.outDir + "/phylotype_table.txt"])
    rc.run_cmd(cmd, logger, options.verbose)

    try:
        os.remove(options.outDir + "/phylotype_table.biom")
    except OSError:
        pass
    cmd = " ".join([BIOM, "convert",
                    "-i", options.outDir + "/phylotype_table.txt",
                    "-o", options.outDir + "/phylotype_table.biom",
                    "--table-type=\"OTU table\" --process-obs-metadata=\"taxonomy\"",
                    "--to-json"])
    rc.run_cmd(cmd, logger, options.verbose)


    # Move representative sequence file to outDir
    shutil.move(tmpDir + "/input_nr_otus_nonchimeras_relabelled.fasta", options.outDir + "/repseqs.fasta")


    # Remove tmp
    if not options.retain:
        logger.info("Cleaning temporary directory")
        shutil.rmtree(tmpDir)


    # Final stats

    #############################
    # Import json formatted OTU #
    #############################

    def biomstats(BIOMFILE):
        import json
        jsondata = open(BIOMFILE)
        biom = json.load(jsondata)

        sampleSize = int(biom["shape"][1])
        otus = int(biom["shape"][0])

        taxonomies = []
        for i in range(len(biom["rows"])):
            taxonomies.append("; ".join(biom["rows"][i]["metadata"]["taxonomy"]))

        sampleids = []
        for i in range(len(biom["columns"])):
            sampleids.append(biom["columns"][i]["id"])

        import numpy as np

        # BIOM table into matrix
        matrix = np.zeros(shape=(otus, sampleSize))
        for i in biom["data"]:
            matrix[i[0], i[1]] = i[2]
        totalCount = matrix.sum()

        return totalCount, otus, sampleSize

    otu_reads_count, otu_count, otu_sample_count = biomstats(options.outDir + "/otu_table.biom")
    phylo_reads_count, phylo_count, phylo_sample_count = biomstats(options.outDir + "/phylotype_table.biom")

    outfile = open(options.outDir + "/summary_pipits_process.txt", "w")

    outfile.write("No.of reads used to generate OTU table: " + str(int(otu_reads_count)) + "\n")
    outfile.write("Number of OTUs:                         " + str(otu_count) + "\n")
    outfile.write("Number of phylotypes:                   " + str(phylo_count) + "\n")
    outfile.write("Number of samples:                      " + str(otu_sample_count) + "\n")

    logger.info(tc.RED + "\tNumber of reads used to generate OTU table: " + str(int(otu_reads_count)) + tc.ENDC)
    logger.info(tc.RED + "\tNumber of OTUs:                             " + str(otu_count) + tc.ENDC)
    logger.info(tc.RED + "\tNumber of phylotypes:                       " + str(phylo_count) + tc.ENDC)
    logger.info(tc.RED + "\tNumber of samples:                          " + str(otu_sample_count) + tc.ENDC)


    # Done!
    logger.info(tc.OKBLUE + "PIPITS_PROCESS ended successfully." + tc.ENDC)
    logger.info(tc.OKGREEN + "Resulting files are in \"" + options.outDir + "\" directory" + tc.ENDC)
